// lexer.cog

struct Lexer
{
	var session: Session;
	var file: SourceFile*;

	var cursor: ConstPtr<Char>;
	var rawStartPtr: UIntPtr;
	var tokenFlags: TokenFlags;
}

func initializeLexer(
	lexer: Lexer*,
	session: Session,
	file: SourceFile*)
{
	lexer.session 	= session;
	lexer.file 	= file;

	lexer.cursor = file.text.begin;
	lexer.tokenFlags =
		kTokenFlag_AtStartOfLine
		| kTokenFlag_AfterSpace;

	lexer.rawStartPtr = cast<UIntPtr>(file.text.begin) - file.firstLoc.raw;
}

func finalizeLexer(
	lexer: Lexer*)
{
}

func  getSink(
	lexer: Lexer*)
	-> DiagnosticSink*
{
	return &lexer.session.sink;
}

func peekByte(lexer: Lexer*) -> Int
{
	return *lexer.cursor;
}

func peekCodePoint(lexer: Lexer*) -> Int
{
	// TODO: UTF-8 decoding
	return peekByte(lexer);
}

func peekLoc(lexer: Lexer*) -> SourceLoc
{
	var loc : SourceLoc;
	loc.raw = cast<UIntPtr>(lexer.cursor) - lexer.rawStartPtr;
	return loc;
}

func advanceByte(lexer: Lexer*)
{
	cogAssert(lexer.cursor != lexer.file.text.end);
	lexer.cursor++;
}

func isAlpha(c: Int) -> Bool
{
	return ((c >= 'a') && (c <= 'z'))
		|| ((c >= 'A') && (c <= 'Z'))
		|| (c == '_');
}

func isDigit(c: Int) -> Bool
{
	return (c >= '0') && (c <= '9');
}

func isAlphaNum(c: Int) -> Bool
{
	return isAlpha(c) || isDigit(c);
}

func isOperatorChar(c: Int) -> Bool
{
	return strchr("~!%^&*-+=|<>?/", c) != 0;
}

func isWhiteSpace(c: Int) -> Bool
{
	return strchr(" \t", c) != 0;
}

func lexDigits(
    lexer: Lexer*,
    base: Int)
{
    for(;;)
    {
        let c = peekByte(lexer);
        switch(c)
        {
        case '0': case '1': case '2': case '3': case '4':
        case '5': case '6': case '7': case '8': case '9':
            advanceByte(lexer);
            continue;

        case 'a': case 'b': case 'c': case 'd': case 'e': case 'f':
        case 'A': case 'B': case 'C': case 'D': case 'E': case 'F':
            if(base > 10)
            {
                advanceByte(lexer);
                continue;
            }
            break;

        default:
            return;
        }
    }
}

func peekNumberExponentMarker(
    lexer: Lexer*,
	base: Int)
	-> Bool
{
    switch(peekByte(lexer))
    {
    case 'e': case 'E':
        return base == 10;

    case 'p': case 'P':
        return base == 16;

    default:
        return false;
    }
}

func lexNumber(
    lexer: Lexer*,
    base: Int)
	-> TokenCode
{
    var code = kTokenCode_IntegerLiteral;

    // lex initial digits
    lexDigits(lexer, base);

    // optional fractional part
    if(peekByte(lexer) == '.')
    {
        advanceByte(lexer);
        code = kTokenCode_FloatingPointLiteral;
        lexDigits(lexer, base);
    }

    // optional exponent part
    if(peekNumberExponentMarker(lexer, base))
    {
        advanceByte(lexer);
        code = kTokenCode_FloatingPointLiteral;

        // optional sign for exponent
        switch(peekByte(lexer))
        {
        case '+': case '-':
            advanceByte(lexer);
            break;

        default:
            break;
        }

        // exponent digits
        lexDigits(lexer, base);
    }

    // TODO: allow any kind of suffix?

    return code;
}

func lexIdentifier(
	lexer: Lexer*)
	-> TokenCode
{
	for(;;)
	{
		let c = peekByte(lexer);
		if(!isAlphaNum(c))
			break;

		advanceByte(lexer);
	}
	return kTokenCode_Identifier;
}

func lexOperator(
	lexer: Lexer*)
	-> TokenCode
{
	for(;;)
	{
		let c = peekByte(lexer);
		if(!isOperatorChar(c))
			break;

		advanceByte(lexer);
	}
	return kTokenCode_InfixOperator;
}

func lexLineComment(
	lexer: Lexer*)
	-> TokenCode
{
	for(;;)
	{
		let c = peekByte(lexer);
		switch(c)
		{
		case '\r': case '\n': case 0:
			return kTokenCode_LineComment;

		default:
			advanceByte(lexer);
			break;
		}
	}
}

func lexBlockComment(
	lexer: Lexer*)
	-> TokenCode
{
	for(;;)
	{
		let c = peekByte(lexer);
		switch(c)
		{
		case 0:
			// end of file: emit an error!
			return kTokenCode_BlockComment;

		case '*':
			advanceByte(lexer);
			switch(peekByte(lexer))
			{
			case '/':
				advanceByte(lexer);
				return kTokenCode_BlockComment;
			default:
				break;
			}
			break;

		default:
			advanceByte(lexer);
			break;
		}
	}
}

func lexWhiteSpace(
	lexer: Lexer*)
	-> TokenCode
{
	for(;;)
	{
		let c = peekByte(lexer);
		if(!isWhiteSpace(c))
		{
			return kTokenCode_WhiteSpace;			
		}

		advanceByte(lexer);
	}
}

func lexStringLiteral(
	lexer: Lexer*,
	delimeter: Int)
{
	for(;;)
	{
		let c = peekByte(lexer);
		if(c == delimeter)
		{
			advanceByte(lexer);
			return;
		}

		switch(c)
		{
		case 0: case '\r': case '\n':
			// TODO: error!
			return;

		case '\\':
			{
				advanceByte(lexer);
				let d = peekByte(lexer);
				switch(d)
				{
				case 0: case '\r': case '\n':
					// TODO: error
					return;
				default:
					// error: unexpected
					advanceByte(lexer);
					break;
				}

			}
			break;

		default:
			advanceByte(lexer);
			continue;
		}

	}
}

func lexTokenImpl(
	lexer: Lexer*)
	-> TokenCode
{
	let c = peekByte(lexer);
	switch(c)
	{
	case 0:
		{
			if(lexer.cursor == lexer.file.text.end)
			{
				return kTokenCode_EndOfFile;
			}
		}
		break;

	case '\r': case '\n':
		advanceByte(lexer);
		{
			let d = peekByte(lexer);
			if((c ^ d) == ('\r' ^ '\n'))
			{
				advanceByte(lexer);
			}
			return kTokenCode_EndOfLine;
		}
		break;

	case ' ': case '\t':
		return lexWhiteSpace(lexer);

	case '\"':
		advanceByte(lexer);
		lexStringLiteral(lexer, c);
		return kTokenCode_StringLiteral;

	case '\'':
		advanceByte(lexer);
		lexStringLiteral(lexer, c);
		return kTokenCode_CharacterLiteral;

	case '/':
		advanceByte(lexer);
		{
			let d = peekByte(lexer);
			switch(d)
			{
			case '/':
				return lexLineComment(lexer);
			case '*':
				return lexBlockComment(lexer);
			default:
				return lexOperator(lexer);
			}
		}
		break;

	case '-':
		advanceByte(lexer);
		{
			let d = peekByte(lexer);
			switch(d)
			{
			case '>':
			// TODO: only if end of operator cluster
				advanceByte(lexer);
				return kTokenCode_Arrow;
			default:
				return lexOperator(lexer);
			}
		}
		break;

	case '=':
		advanceByte(lexer);
		{
			let d = peekByte(lexer);
			if(isOperatorChar(d))
			{
				return lexOperator(lexer);
			}
			else
			{
				return kTokenCode_Assign;
			}
		}
		break;

	case '.':
		advanceByte(lexer);
		if(peekByte(lexer) == '.')
		{
			// this is an operator
			for(;;)
			{
				advanceByte(lexer);
				if(peekByte(lexer) != '.')
					break;
			}
			return kTokenCode_InfixOperator;
		}
		else
		{
			return kTokenCode_Dot;
		}

    case '0':
        advanceByte(lexer);
        {
            let d = peekByte(lexer);
            switch(d)
            {
            case 'x': case 'X':
                advanceByte(lexer);
                return lexNumber(lexer, 16);

            case 'b': case 'B':
                advanceByte(lexer);
                return lexNumber(lexer, 2);

            default:
                return lexNumber(lexer, 10);
            }
        }

              case '1': case '2': case '3': case '4':
    case '5': case '6': case '7': case '8': case '9':
        return lexNumber(lexer, 10);

    case '@':
        advanceByte(lexer);
        return lexIdentifier(lexer);

/*
#define SINGLE_CHAR_TOKEN(ch, code) \
	case ch: advanceByte(lexer); return kTokenCode_##code

	SINGLE_CHAR_TOKEN('{', LCurly);
	SINGLE_CHAR_TOKEN('}', RCurly);
    SINGLE_CHAR_TOKEN('[', LSquare);
    SINGLE_CHAR_TOKEN(']', RSquare);
    SINGLE_CHAR_TOKEN('(', LParen);
	SINGLE_CHAR_TOKEN(')', RParen);
	SINGLE_CHAR_TOKEN(';', Semi);
	SINGLE_CHAR_TOKEN(':', Colon);
	SINGLE_CHAR_TOKEN(',', Comma);

#undef SINGLE_CHAR_TOKEN
*/
	case '{': advanceByte(lexer); return kTokenCode_LCurly;
	case '}': advanceByte(lexer); return kTokenCode_RCurly;
	case '[': advanceByte(lexer); return kTokenCode_LSquare;
	case ']': advanceByte(lexer); return kTokenCode_RSquare;
	case '(': advanceByte(lexer); return kTokenCode_LParen;
	case ')': advanceByte(lexer); return kTokenCode_RParen;
	case ';': advanceByte(lexer); return kTokenCode_Semi;
	case ':': advanceByte(lexer); return kTokenCode_Colon;
	case ',': advanceByte(lexer); return kTokenCode_Comma;

	default:
		break;
	}
	if(isAlpha(c))
	{
		return lexIdentifier(lexer);
	}
	else if(isOperatorChar(c))
	{
		return lexOperator(lexer);
	}
	else
	{
		diagnose(getSink(lexer),
			peekLoc(lexer),
			kDiagnostic_unexpectedCharacter,
			peekCodePoint(lexer));
		advanceByte(lexer);
		return kTokenCode_Invalid;
	}
}

func lexToken(
	lexer: Lexer*)
	-> Token
{
	for(;;)
	{

		let begin = lexer.cursor;
		let flags = lexer.tokenFlags;
		let code = lexTokenImpl(lexer);
		let end = lexer.cursor;
		let rawData = cast<Ptr<Void> >(begin);

		switch(code)
		{
		case kTokenCode_Identifier:
		case kTokenCode_InfixOperator:
		case kTokenCode_PrefixOperator:
		case kTokenCode_PostfixOperator:
			rawData = getName(lexer.session, StringSpan(begin, end));
			// fall through
		default:
			lexer.tokenFlags = 0;
			break;

		case kTokenCode_LineComment:
			continue;

		case kTokenCode_EndOfLine:
			lexer.tokenFlags |= kTokenFlag_AtStartOfLine;
		case kTokenCode_WhiteSpace:
		case kTokenCode_BlockComment:
			lexer.tokenFlags |= kTokenFlag_AfterSpace;
			continue;
		}

		var token : Token;
		token.code = code;
		token.flags = flags;
		token.rawData = rawData;
		token.rawSize = end - begin;
		token.rawLoc = cast<UIntPtr>(begin) - lexer.rawStartPtr;
		token.advance = 0;
		return token;
	}
}

func matchDelimeter(
	lexer: Lexer*,
	open: Token*,
	begin: Token*,
	end: ConstPtr<Token>,
	closingTokenCode: TokenCode)
	-> Token* 
{
	var cursor = begin;
	while(cursor != end)
	{
		let code = cursor.code;
		if(code == closingTokenCode)
		{
			// We found the closing token

			// Record the "advance" field for the opening token
			open.advance = uint32_t( (cursor - open) - 1 );

			return cursor;
		}

		switch(cursor.code)
		{
		default:
			cursor = cursor + 1;
			break;

		// TODO: need to detect extra closing delimeters here
		// and report an error

		case kTokenCode_LParen:
			cursor = matchDelimeter(lexer, cursor, cursor+1, end, kTokenCode_RParen);
			break;

		case kTokenCode_LCurly:
			cursor = matchDelimeter(lexer, cursor, cursor+1, end, kTokenCode_RCurly);
			break;

		case kTokenCode_LSquare:
			cursor = matchDelimeter(lexer, cursor, cursor+1, end, kTokenCode_RSquare);
			break;
		}
	}

	// unmatched! need to emit an error!

	return cursor;
}

func matchDelimeters(
	lexer: Lexer*,
	begin: Token*,
	end: ConstPtr<Token>)
{
	matchDelimeter(lexer, 0, begin, end, kTokenCode_EndOfFile);
}

func markUpOperators(
	lexer: Lexer*,
	begin: Token*,
	end: ConstPtr<Token>)
{
	var afterSpace = true;

	var cursor = begin;
	while(cursor != end)
	{
		let code = cursor.code;
		let nextCode = (cursor + 1).code;

		if(code == kTokenCode_InfixOperator)
		{
			if(cursor.flags & kTokenFlag_AfterSpace)
			{
				afterSpace = true;
			}

			var beforeSpace = ((cursor + 1).flags & kTokenFlag_AfterSpace) != 0;
			switch(nextCode)
			{
			default:
				break;

			case kTokenCode_RParen:
			case kTokenCode_RSquare:
			case kTokenCode_RCurly:
			case kTokenCode_Semi:
			case kTokenCode_Colon:
			case kTokenCode_Comma:
				beforeSpace = true;
			}

			if(beforeSpace == afterSpace)
			{
				// do nothing, and leave infix
			}
			else if(beforeSpace)
			{
				cursor.code = kTokenCode_PostfixOperator;
			}
			else
			{
				cursor.code = kTokenCode_PrefixOperator;
			}
		}

		switch(code)
		{
		default:
			afterSpace = false;
			break;

		case kTokenCode_LParen:
		case kTokenCode_LSquare:
		case kTokenCode_LCurly:
		case kTokenCode_Semi:
		case kTokenCode_Colon:
		case kTokenCode_Comma:
			afterSpace = true;
			break;
		}

		cursor++;
	}
}

func lexTokens(
	session: Session,
	file: SourceFile*)
	-> TokenSpan
{
	var lexer : Lexer;
	initializeLexer(&lexer, session, file);

	var tokenLimit : SizeT = 16;
	var tokens = cast<Ptr<Token> >(malloc(tokenLimit * sizeOf<Token>()));
	var tokenCount : SizeT = 0;

	for(;;)
	{
		let token = lexToken(&lexer);

		if(tokenCount == tokenLimit)
		{
			tokenLimit *= 2;
			tokens = cast<Ptr<Token> >(realloc(tokens, tokenLimit*sizeOf<Token>()));
		}
		tokens[tokenCount++] = token;

		if(token.code == kTokenCode_EndOfFile)
			break;
	}

	// trim allocation to fit
	tokens = cast<Ptr<Token> >(realloc(tokens, tokenLimit*sizeOf<Token>()));

	// end-of-file token is at the end of the range
	let endToken = tokens + tokenCount - 1;

	let tokenSpan = TokenSpan(tokens, endToken);

	matchDelimeters(&lexer, tokens, endToken);

	markUpOperators(&lexer, tokens, endToken);

	finalizeLexer(&lexer);

	return tokenSpan;
}
